{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ab9283",
   "metadata": {
    "id": "24QCUtRjshW-"
   },
   "source": [
    "# Regression \n",
    "\n",
    "Reading: Chapter 6\n",
    "\n",
    "This module focuses on a particular class of supervised machine learning: regression, where we are trying to discover relationships between variables in order to predict continuous quantities.  Content for this module:\n",
    "* linear least-squares \n",
    "* non-linear regression models\n",
    "* penalized regression models\n",
    "\n",
    "Resources:\n",
    "\n",
    "1. Gander, Gander and Kwok (2014), Scientific Computing: An introduction using Maple and MATLAB, Springer, https://www.springer.com/us/book/9783319043241\n",
    "2. Kuhn and Johnson (2013), Applied Predictive Modeling, Springer\n",
    "https://www.springer.com/us/book/9781461468486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8451c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from scipy.linalg import solve_triangular\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e463fbba",
   "metadata": {
    "id": "NJ1BP0c3shW_"
   },
   "source": [
    "## Linear Regression (Section 6.2) \n",
    "\n",
    "For samples with $D$ features, linear regression attempts to fit a $D$-dimensional hyperplane to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b2c42",
   "metadata": {
    "id": "COuDQWdWshXA"
   },
   "outputs": [],
   "source": [
    "# Generate some data \n",
    "n = 200\n",
    "x = np.random.rand(n,1) # feature 1\n",
    "y = np.random.rand(n,1) # feature 2\n",
    "z = x + 0.5*y + 0.1*np.random.randn(n,1) + 0.2 # response variable, z(x,y)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x,y,z,c='b',marker='o')\n",
    "ax.set_xlabel('feature 1')\n",
    "ax.set_ylabel('feature 2')\n",
    "_ = ax.set_zlabel('response')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4908e6b",
   "metadata": {
    "id": "VQW9RolDshXE"
   },
   "source": [
    "Specifically, we are trying to find coefficients $a_0, a_1, a_2, \\ldots, a_D$ so that the hyperplane\n",
    "\n",
    "\\begin{align}\n",
    " z(x_1,x_2,\\ldots,x_D) = a_0 + a_1 x_1 + a_2 x_2 + \\cdots + a_D x_D\n",
    "\\end{align}\n",
    "\n",
    "models the response variable. (note, this is the generalized form of equation (6.2) in textbook).  Suppose we have $N$ samples, each with $D$ features, $\\vec{x}_i = [{x_i}_1,{x_i}_2,\\ldots,{x_i}_D],\\, i= 1,\\ldots N$. Then the hope, is that \n",
    "\n",
    "\\begin{align}\n",
    "  z({x_i}_1,{x_i}_2,\\ldots,{x_i}_D) \\approx y_i,\\quad \\text{for } i = 1,\\ldots, N\n",
    "\\end{align}\n",
    "\n",
    "This gives rise to an over-determined system of equations:\n",
    "\n",
    "\\begin{align}\n",
    "  a_0 + a_1 x_{11} + a_2 x_{12} + \\cdots + a_D x_{1D} &\\approx y_1\\\\\n",
    "  a_0 + a_1 x_{21} + a_2 x_{22} + \\cdots + a_D x_{2D} &\\approx y_2\\\\\n",
    "    &\\vdots \\\\\n",
    "  a_0 + a_1 x_{N1} + a_2 x_{N2} + \\cdots + a_D x_{ND} &\\approx y_N\\\\\n",
    "\\end{align}\n",
    "\n",
    "This is more easily expressed in matrix form, $X\\,a = y$,\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1D}\\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2D}\\\\\n",
    "1 & x_{31} & x_{32} & \\cdots & x_{3D}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_{N1} & X_{x2} & \\cdots & x_{ND}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "a_0\\\\\n",
    "a_1 \\\\\n",
    "\\vdots\\\\\n",
    "a_D\n",
    "\\end{bmatrix}\n",
    "\\approx\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3 \\\\\n",
    "\\vdots \\\\\n",
    "y_N\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Here, the matrix $X$ is of size: $N \\times (D+1)$, the vector of coefficients we seek is of size $(D+1)\\times 1$, the right-hand side vector is of size $N \\times 1$.  Here, we are imagining that $N \\gg D$.  How do we solve the overdetermined system?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba0734",
   "metadata": {
    "id": "r3HjcoNkshXE"
   },
   "source": [
    "Well, it turns out we can't exactly most of the time, i.e., we can not find $a$ such that $X\\,a =y$.  There is always some mismatch, a residual: $r = X\\,a - y$.  We want to minimize this residual, for example, minimize $\\|r\\|^2_2 = \\|X\\,a - y\\|^2_2$, i.e.,\n",
    "\n",
    "$$ \\min \\|X\\,a - y \\|^2_2 = \\min \\left( \\sum_{i = 1}^N  (X\\,a - y)_i^2 \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c5516d",
   "metadata": {
    "id": "FgoWGyNtshXF"
   },
   "source": [
    "#### Theorem \n",
    "\n",
    "If $X^T (X\\,a - y) = 0$, then $a$ solves the linear least squares problem, i.e. $a$ minimizes $\\|X\\,a - y\\|_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1f243",
   "metadata": {
    "id": "HSkpTj-gshXG"
   },
   "source": [
    "Proof: Let $c$ be any vector of size $(D+1)\\times 1$.  Then $X(a+c) - y = X\\,a -y + X\\,c $, and\n",
    "\n",
    "\\begin{align}\n",
    "\\|X(a+c) - y\\|_2^2 &= \\left( X\\,a -y + X\\,c \\right)^T \\left(X\\,a -y + X\\,c \\right) \\\\\n",
    "&= (X\\,a - y)^T (X\\,a -y) + 2 (X\\,c)^T(X\\,a - y) + (X\\,c)^T (X\\,c) \\\\\n",
    "&= \\|X\\,a - y\\|_2^2 + 2c^T X^T (X\\,a - y) + \\|X\\,c\\|_2^2\n",
    "\\end{align}\n",
    "\n",
    "If $X^T (X\\,a - y) = 0$, then\n",
    "\\begin{align}\n",
    "\\|X(a+c) - y\\|_2^2 = \\|X\\,a - y\\|_2^2 + \\|X\\,c\\|_2^2\n",
    "\\end{align}\n",
    "\n",
    "Hence, for every $c$, we have \n",
    "\\begin{align}\n",
    "\\|X(a+c) - y\\|_2^2 \\ge  \\|X\\,a - y\\|_2^2\n",
    "\\end{align}\n",
    "\n",
    "so $a$ must minimize $\\|X\\,a - y\\|_2^2$ as required. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec653df",
   "metadata": {
    "id": "1TJAqxM-shXG"
   },
   "source": [
    "The equation $X^T(X\\,a - y) = 0$ is often written as\n",
    "\n",
    "$$ X^T X\\,a = X^T y, $$\n",
    "\n",
    "called the normal equations.  There is a geometric interpretation: the vector in the range (column space of $X$) that lies closest to $y$ makes $X\\,a - y$ perpendicular to the range.  Note, \n",
    "* the normal equations expresses the $N\\times(D+1)$ linear least squares problem as a $(D+1)\\times (D+1)$ linear system.\n",
    "* the matrix $X^T X$ is symmetric\n",
    "* $X^T X$ is singular . if and only if the columns of $X$ are linearly dependent, i.e., the rank of $X$ is less than $(D+1)$\n",
    "* if $X^T X$ is non singular, then it is positive definite.\n",
    "\n",
    "Observe that if $X^T X$ is non singular, then\n",
    "$$ X^T X\\,a = X^T y, $$\n",
    "can be written as \n",
    "$$ a = (X^T X)^{-1} X^T y, $$\n",
    "\n",
    "One sometimes defines the pseudo inverse,\n",
    "$$ X^\\dagger = (X^T X)^{-1} X^T.$$\n",
    "Then, the least squares problem $X\\,a = y$ has a solution $a = X^\\dagger y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d082b68b",
   "metadata": {
    "id": "Lz2OT2JCshXH"
   },
   "source": [
    "### QR Factorization \n",
    "\n",
    "Numerically, one never forms the pseudo inverse to solve the least squares problem.  Rather, we rely on a useful factorization known as the QR factorization.  Every matrix has a QR factorization, $X = QR$, where $Q$ is an orthogonal matrix, and $R$ is upper triangular.  For convenience, lets refer to\n",
    "* $X$ as an $m \\times n$ matrix\n",
    "* $Q$ is an $m \\times m$ matrix, whose columns are orthonormal to each other, i.e.:\n",
    "    * $q_i \\cdot q_j = 0$ if $i\\neq j$\n",
    "    * $q_i \\cdot q_i = 1, \\quad i=1,2,\\ldots,m$\n",
    "* $R$ is an $m \\times n$ upper triangular matrix, i.e., $r_{ij} = 0$ if $i > j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a5fc65",
   "metadata": {
    "id": "IN4ZYJlUshXH"
   },
   "source": [
    "#### Example 1\n",
    "Lets switch to a toy example where we have an over-determined system, and explore the QR factorization and least-squares solution.  We will return to regression shortly. Suppose we have two packages that we wish to ship.  We measure the weight of these two packages in the office.  Package $A$ measures in at 2 pounds, package $B$ measures in at 5 pounds.  At the distribution site however, the two packages are weighed together, and the joint weight is reported at 8 pounds.  Use a least-squares solution to find a best estimate for the weight of each package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb5300",
   "metadata": {
    "id": "xEMwSBqVshXI"
   },
   "outputs": [],
   "source": [
    "X = np.array( [ [1,0], [0,1], [1,1]])\n",
    "print(\"our matrix X:\")\n",
    "print(X)\n",
    "y = np.array( [[2],[5],[8]])\n",
    "print(\"our target y:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf8c67",
   "metadata": {
    "id": "uXwx7n6oshXQ"
   },
   "outputs": [],
   "source": [
    "[Q,R] = np.linalg.qr(X, mode = 'complete')\n",
    "print(\"Lets check that Q*R = X, up to machine precision\")\n",
    "print(np.matmul(Q,R))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505ef91",
   "metadata": {
    "id": "0-ieVSlmshXT"
   },
   "source": [
    "Lets check that $Q$ has orthonormal columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a391c88b",
   "metadata": {
    "id": "41TZFbK0shXT"
   },
   "outputs": [],
   "source": [
    "print(\"matrix Q: \")\n",
    "print(Q)\n",
    "print(\"\\nVarious inner products:\")\n",
    "print(\"q1^t q_1 = %g\\n \" % np.dot(Q[:,0],Q[:,0]) )\n",
    "print(\"q1^t q_2 = %g \\n\" % np.dot(Q[:,0],Q[:,1]) )\n",
    "print(\"q1^t q_3 = %g \\n\" % np.dot(Q[:,0],Q[:,2]) )\n",
    "print(\"q2^t q_2 = %g \\n\" % np.dot(Q[:,1],Q[:,1]) )\n",
    "print(\"q2^t q_3 = %g \\n\" % np.dot(Q[:,1],Q[:,2]) )\n",
    "print(\"q3^t q_3 = %g \" % np.dot(Q[:,2],Q[:,2]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742a4583",
   "metadata": {
    "id": "N-melqZZshXW"
   },
   "source": [
    "Lets check that $R$ is upper triangular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdff85a2",
   "metadata": {
    "id": "-SdEwcLlshXW"
   },
   "outputs": [],
   "source": [
    "print(\"R = \")\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c332cb00",
   "metadata": {
    "id": "ww6ariM1shXa"
   },
   "source": [
    "There is a reduced factorization that is more useful. $ X = \\hat{Q} \\hat{R}$, where $\\hat{Q}$ is an $m\\times n$ matrix with orthonormal columns, and $\\hat{R}$ is an $n\\times n$ upper triangular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb81f45",
   "metadata": {
    "id": "tg_UCEGvshXb"
   },
   "outputs": [],
   "source": [
    "[Q,R] = np.linalg.qr(X, mode = 'reduced')\n",
    "print(\"Q = \")\n",
    "print(Q)\n",
    "print(\"R = \")\n",
    "print(R)\n",
    "print(\"Lets check that Q*R = X, up to machine precision\")\n",
    "print(np.matmul(Q,R))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a4a40",
   "metadata": {
    "id": "ZoAELNERshXd"
   },
   "source": [
    "### QR Factorization + Normal Equations \n",
    "\n",
    "How do we use the $QR$ factorization to solve the normal equations?  Suppose we have $X = \\hat{Q}\\,\\hat{R}$.  Observe:\n",
    "\\begin{align}\n",
    "X^T X a &= X^T y \\\\\n",
    "(\\hat{Q}\\,\\hat{R})^T (\\hat{Q}\\,\\hat{R})\\, a &= (\\hat{Q}\\,\\hat{R})^T y \\\\\n",
    "\\hat{R}^T \\hat{Q}^T \\hat{Q}\\, \\hat{R} \\, a &= \\hat{R}^T \\hat{Q}^T y\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a6d292",
   "metadata": {
    "id": "qyWMef78shXe"
   },
   "source": [
    "If $\\hat{R}^T$ exists, and observing that $\\hat{Q}^T \\hat{Q}$ gives an $n\\times n$ identity matrix,\n",
    "\\begin{align}\n",
    "&\\implies \\hat{R}^T \\hat{R} \\, a = \\hat{R}^T \\hat{Q}^T y \\\\\n",
    "&\\implies  \\hat{R} \\, a = \\hat{Q}^T y\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fbbc2a",
   "metadata": {
    "id": "CPEANvuFshXe"
   },
   "source": [
    "#### Example 1 (revisited)\n",
    "Lets use our toy example to solve our derived normal equations, simplified using the QR factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a85f6",
   "metadata": {
    "id": "u606satHshXf"
   },
   "outputs": [],
   "source": [
    "# from scipy.linalg import solve_triangular\n",
    "\n",
    "x = solve_triangular(R,np.dot(np.transpose(Q),y)) \n",
    "# default is to solve upper triangular matrix.  to solve lower triangular, add flag lower=true\n",
    "print(\"package weights: \")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a18960",
   "metadata": {
    "id": "HeZHiMbGshXh"
   },
   "source": [
    "#### Example 2 \n",
    "\n",
    "Let's now visit regression, which is a linear least squares problem, using the diabetes dataset that is part of Scikit-learn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b94cd",
   "metadata": {
    "id": "YUgZvxk4shXi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn import datasets, linear_model\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5dddec",
   "metadata": {
    "id": "LUzQlbJzshXl"
   },
   "source": [
    "Lets pick one feature to do a linear regression with.  (We will explore using more features shortly).  The third column is the body-mass-index, centered about the mean and scaled by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22142da",
   "metadata": {
    "id": "FfSErkl3shXl"
   },
   "outputs": [],
   "source": [
    "X = diabetes.data[:,np.newaxis,2]\n",
    "y = diabetes.target[:]\n",
    "\n",
    "plt.scatter(X,y,color='black')\n",
    "plt.xlabel('scaled BMI')\n",
    "plt.ylabel('diabetes progression after one year');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0565de",
   "metadata": {
    "id": "eLj37bP1shXn"
   },
   "source": [
    "Lets split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea60dfd",
   "metadata": {
    "id": "XfdVdCUQshXo"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "              \n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# train regression model\n",
    "regr = LinearRegression(fit_intercept=True)\n",
    "regr.fit(X_train,y_train)\n",
    "\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# plot training data, overlay with fit\n",
    "plt.scatter(X_train,y_train,color='black')\n",
    "plt.plot(X_test,y_pred,color='blue',linewidth=3)\n",
    "plt.xlabel('scaled BMI')\n",
    "_ = plt.ylabel('diabetes progression after one year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4ff3d1",
   "metadata": {
    "id": "tG96PIaxshXr"
   },
   "source": [
    "We should of course measure the mean squared error (MSE) and the $R^2$ value.  Denote the true target as $y$, the predicted target as $\\hat{y}$.  Then,\n",
    "\n",
    "\\begin{align}\n",
    "  \\text{MSE} = \\frac{1}{n} \\sum_{i = 1}^n (y_{i} - \\hat{y}_{i})^2.\n",
    "\\end{align}\n",
    "\n",
    "What about the $R^2$ value? If the mean can be estimated or computed,\n",
    "\n",
    "\\begin{align}\n",
    "  \\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i,\n",
    "\\end{align}\n",
    "\n",
    "then the $R^2$ value can be computed using \n",
    "\n",
    "\\begin{align}\n",
    "R^2 = 1 - \\frac{\\sum_{i = 1}^n (y_{i} - \\hat{y}_{i})^2} {\\sum_{i = 1}^n (y_{i} - \\bar{y})^2}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfef654f",
   "metadata": {
    "id": "iskfJApvshXs"
   },
   "source": [
    "How do we interpret the $R^2$ value?\n",
    "* $R^2 = 0$ indicates that the model explains none of the variability of the response data about the mean\n",
    "* $R^2 = 1$ indicates that the model explains all the variability of response data about the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d08c58",
   "metadata": {
    "id": "1YaMxuXWshXs"
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# R^2 score: 1 is perfect prediction\n",
    "print('R^2 score: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17079004",
   "metadata": {
    "id": "EL2Y7bSashXv"
   },
   "source": [
    "The coefficients of the blue line can be obtained using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfbfdb7",
   "metadata": {
    "id": "Es3iJCtGshXv"
   },
   "outputs": [],
   "source": [
    "print('Coefficients: %g '%  regr.coef_)\n",
    "print('Intercept: %g '%  regr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a859ee2",
   "metadata": {
    "id": "pIBy7KFHshXy"
   },
   "source": [
    "Lets recover these coefficients using our normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283e2f52",
   "metadata": {
    "id": "5PK2Id6UshXz"
   },
   "outputs": [],
   "source": [
    "A = np.ones( (X_train.size,2) )\n",
    "A[:,np.newaxis,1]  = X_train\n",
    "\n",
    "# find QR factorization\n",
    "[Q,R] = np.linalg.qr(A, mode = 'reduced')\n",
    "\n",
    "# solve normal equations\n",
    "coeff = solve_triangular(R,np.dot(np.transpose(Q),y_train)) \n",
    "# default is to solve upper triangular matrix.  to solve lower triangular, add flag lower=true\n",
    "\n",
    "# print coefficients\n",
    "print(\"coefficients: \")\n",
    "print(coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1044bd1c",
   "metadata": {
    "id": "1oLBHGLDshX1"
   },
   "source": [
    "Voila!  They match. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d309d0",
   "metadata": {
    "id": "NmzYZpIJshX1"
   },
   "source": [
    "#### Example 3\n",
    "\n",
    " We can also fit more features; hard to visualize when there are more than 2 fields.  Lets try: age and blood pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe2064",
   "metadata": {
    "id": "4CMD12aLshX2"
   },
   "outputs": [],
   "source": [
    "X = diabetes.data[:,[2,3]]\n",
    "y = diabetes.target[:]\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:,0],X[:,1],y,c='black',marker='o')\n",
    "ax.set_xlabel('scaled BMI')\n",
    "ax.set_ylabel('scaled blood pressure')\n",
    "_ = ax.set_zlabel('diabetes progression after one year');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc0124",
   "metadata": {
    "id": "iUziN9pnshX5"
   },
   "source": [
    "Let's split the data into training and test data, and fit a hyperplane to this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b1151",
   "metadata": {
    "id": "FNNIJsacshX5"
   },
   "outputs": [],
   "source": [
    "# split data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "              \n",
    "# train regression model on training data\n",
    "regr = LinearRegression(fit_intercept=True)\n",
    "regr.fit(X_train,y_train)\n",
    "\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_train[:,0],X_train[:,1],y_train,c='black',marker='o')\n",
    "ax.plot_trisurf(X_test[:,0],X_test[:,1],y_pred) # plot hyperplane\n",
    "ax.set_xlabel('scaled BMI')\n",
    "ax.set_ylabel('scaled blood pressure')\n",
    "_ = ax.set_zlabel('diabetes progression after one year')\n",
    "\n",
    "print('Coefficients: ')\n",
    "print(regr.coef_)\n",
    "print('Intercept: %g '%  regr.intercept_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# R^2 score: 1 is perfect prediction\n",
    "print('R^2 score: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0857a094",
   "metadata": {
    "id": "h-kCIjeqshX8"
   },
   "source": [
    "Again, lets recover the hyperplane coefficients using our normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3104872",
   "metadata": {
    "id": "2B0dFWzwshX8"
   },
   "outputs": [],
   "source": [
    "A = np.ones( (X_train.shape[0],3) )\n",
    "A[:,np.newaxis,[1,2]]  = X_train[:,np.newaxis,[0,1]]\n",
    "\n",
    "# find QR factorization\n",
    "[Q,R] = np.linalg.qr(A, mode = 'reduced')\n",
    "\n",
    "# solve normal equations\n",
    "coeff = solve_triangular(R,np.dot(np.transpose(Q),y_train)) \n",
    "# default is to solve upper triangular matrix.  to solve lower triangular, add flag lower=true\n",
    "\n",
    "# print coefficients \n",
    "print(\"coefficients: \")\n",
    "print(coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1268847",
   "metadata": {
    "id": "Fvlk_YiTshX-"
   },
   "source": [
    "#### Example 4 \n",
    "\n",
    "We can of course fit a hyperplane to all the features, though it is difficult to now visualize the 10-dimensional hyperplane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170435e1",
   "metadata": {
    "id": "qWFsuEUEshX_"
   },
   "outputs": [],
   "source": [
    "X = diabetes.data\n",
    "y = diabetes.target[:]\n",
    "\n",
    "# split data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "              \n",
    "# train regression model on training data\n",
    "regr = LinearRegression(fit_intercept=True)\n",
    "regr.fit(X_train,y_train)\n",
    "\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "print('Coefficients: ')\n",
    "print(regr.coef_)\n",
    "print('Intercept: %g '%  regr.intercept_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# R^2 score: 1 is perfect prediction\n",
    "print('R^2 score: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9633290",
   "metadata": {
    "id": "qE1nLC4rshYB"
   },
   "source": [
    "and again, using our normal equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c04a5",
   "metadata": {
    "id": "lvnRm3FpshYC"
   },
   "outputs": [],
   "source": [
    "A = np.ones( (X_train.shape[0],11) )\n",
    "A[:,np.newaxis,range(1,11)]  = X_train[:,np.newaxis,range(10)]\n",
    "\n",
    "# find QR factorization\n",
    "[Q,R] = np.linalg.qr(A, mode = 'reduced')\n",
    "\n",
    "# solve normal equations\n",
    "coeff = solve_triangular(R,np.dot(np.transpose(Q),y_train)) \n",
    "# default is to solve upper triangular matrix.  to solve lower triangular, add flag lower=true\n",
    "\n",
    "# print coefficients\n",
    "print(\"coefficients: \")\n",
    "print(coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3438286d",
   "metadata": {
    "id": "2Da04ia2shYF"
   },
   "source": [
    "Going back to first principle allows you to fit more general functions to your data,\n",
    "\n",
    "\\begin{align}\n",
    "y(\\vec{x}) = a_1 f_1(\\vec{x}) + a_2 f_2(\\vec{x}) + \\cdots + a_D f_D(\\vec{x})\n",
    "\\end{align}\n",
    "\n",
    "as long as we seek unknown coefficients that give a linear combination of these functions.  Examples of these more general functions include higher degree polynomials, periodic functions such as sines and cosines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b8ce8",
   "metadata": {
    "id": "wyMzT9FNshYF"
   },
   "source": [
    "## Sparse Models (Section 6.2.3) \n",
    "\n",
    "If one has many features (predictors), there is a tendency to over-fit the data: one picks up noise in the system if there are features that are uninformative.    Feature selection seeks to select only the informative features while discarding non-informative ones.  The text introduces one approach known as penalization methods, where one penalizes the magnitude of coefficients of features.  Recall that in the least squares sense, we wish to minimize\n",
    "$\\|r\\|^2_2 = \\|X\\,a - y\\|^2_2$, i.e.,\n",
    "\n",
    "$$ \\min \\|X\\,a - y \\|^2_2 = \\min \\left( \\sum_{i = 1}^N  (X\\,a - y)_i^2 \\right) $$\n",
    "\n",
    "Lasso Regression adds $L_1$ regularization to the minimization problem, that is, it adds a penalty equivalent to the sum of the absolute value of the coefficients\n",
    "\n",
    "$$ \\min \\left(\\|X\\,a - y \\|^2_2 + \\alpha \\|a\\|_1\\right) = \\min \\left(\\left( \\sum_{i = 1}^N  (X\\,a - y)_i^2 \\right) + \\alpha \\sum_{j=0}^{D} |a_j| \\right)$$\n",
    "\n",
    "Ridge Regression adds $L_2$ regularization to the minimization problem, that is, it adds a penalty equivalent to the sum of the squares of the coefficients\n",
    "\n",
    "$$ \\min \\left(\\|X\\,a - y \\|^2_2 + \\alpha \\|a\\|_2^2\\right) = \\min \\left(\\left( \\sum_{i = 1}^N  (X\\,a - y)_i^2 \\right) + \\alpha \\sum_{j=0}^{D} |a_j|^2 \\right)$$\n",
    "\n",
    "There are courses that cover numerical methods for solving these sort of optimization problems (MA 5630); we will not try to explain them here.  Instead, lets focus on one of these methods to better understand the role of the regularization parameter and why these penalization methods fall into the class of feature selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e56bb",
   "metadata": {
    "id": "3OvIw2Da_O_T"
   },
   "source": [
    "### Example 5\n",
    "\n",
    "Lets use the diabetes data, all features, and apply Lasso Regression with various penalty coefficients, $\\lambda$, and track how the coefficients of the hyperplane, the MSE and the $R^2$ values changes.  Note, if $\\alpha = 0$ (no penalization), we recover exactly the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c8c97",
   "metadata": {
    "id": "UXA29xsD_JdK"
   },
   "outputs": [],
   "source": [
    "X = diabetes.data\n",
    "y = diabetes.target[:]\n",
    "\n",
    "# split data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# test the following values of lambda\n",
    "alpha = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "# We'll store our data in a pandas data frame\n",
    "import pandas as pd\n",
    "\n",
    "#Initialize a dataframe to store the results:\n",
    "col = ['MSE','R^2'] + ['a_%d'%i for i in range(11)]\n",
    "ind = ['alpha = %g'%i for i in alpha]\n",
    "       \n",
    "results = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "for i in range(len(alpha)):\n",
    "    # train regression model on training data\n",
    "    regr = linear_model.Ridge(fit_intercept=True,alpha=alpha[i])\n",
    "    regr.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = regr.predict(X_test)\n",
    "    \n",
    "    results.iloc[i,2] = regr.intercept_\n",
    "    results.iloc[i,3:] = regr.coef_\n",
    "    results.iloc[i,0] = mean_squared_error(y_test, y_pred)\n",
    "    results.iloc[i,1] = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59bf846",
   "metadata": {
    "id": "Vmur_Vjg_XWb"
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e4edef",
   "metadata": {
    "id": "V7qFjcMz_mqB"
   },
   "source": [
    "### Try on your own \n",
    "\n",
    "Exercise: \n",
    "* explore ridge regression instead of lasso regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e9255",
   "metadata": {
    "id": "1SOXS5iC_Z5K"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
