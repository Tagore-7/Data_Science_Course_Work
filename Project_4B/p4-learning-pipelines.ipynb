{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFivbbl2_nYK"
   },
   "source": [
    "# Learning - Pipelines \n",
    "\n",
    "\n",
    "The project explores using classification models for various tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yyx4tHW5A0dT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(5550)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X99MnLn9rApw"
   },
   "source": [
    "# Model Evaluation (Review)\n",
    "\n",
    "*Materials copied or adpated from Applied Machine Learning in Python by Mueller*\n",
    "\n",
    "Let's review the different model evalution approaches starting from the simplistic and understand their limitations. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9EUNFX-xcO5"
   },
   "source": [
    "## 1 - Hold-out set \n",
    "\n",
    "Let's start with a simplisted approach to model evaluation, split the data into training and testing data.  \n",
    "\n",
    "<img src=\"https://pages.mtu.edu/~lebrown/un5550-f21/p6/train_test_split_new.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "This is a common approach, but has multiple limitations.  \n",
    "\n",
    "#### *How to solve this problem?*   \n",
    "\n",
    "**A** Use an additional hold-out set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WYUq94UyWeD"
   },
   "source": [
    "## 2 - Three-fold split or Train/Validation/Test set \n",
    "\n",
    "Use of three separate sets:    \n",
    "* the training set for model building \n",
    "* the validation set for model selection \n",
    "* the test set for final model evaluation \n",
    "is probably the most common used method for model selection and evaluation. It is a **best practice** to follow (along with other techniques described below). \n",
    "\n",
    "<img src=\"https://pages.mtu.edu/~lebrown/un5550-f21/p6/train_test_validation_split.png\" width=\"50%\">\n",
    "\n",
    "With this new approach, we use the validation set to select the optimum hyper-paramter and the test set to estimate the performance (accuracy).  Because the test set was not used for estimating the best hyper-parameter, the test set provides an unbiased estimate of the generalization performance. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHJb1KIS0NUY"
   },
   "source": [
    "### Example of three-fold split \n",
    "\n",
    "Let's see a simple example of using the three-fold split to select the number of neighbors in KNN on the iris data set.  We first take 25% as the test set, then take 25% of the remaining as the validation set (about ~19% of the original data).  \n",
    "\n",
    "We build a model for each value of `n_neighbors` (range from 1-15 in steps of 2), evaluate it on the validation set and store the result.  We find the value which gives the best performance. \n",
    "\n",
    "Then, we often rebuild the model on all the training data (train + validation) with the best-performing hyper-parameter (as determined by the validation set), and evaluate the model on the test set. \n",
    "\n",
    "The step of retraining the model using bot the training and validation set is optional, in particular, if model training is very expensive or if the amount of training data is large enough for our model.  In this example problem, neither is the case so we retrain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5L5ZeC8316eE",
    "outputId": "3474241d-9035-4299-9fb3-a416ce6647eb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation score:  1.000\n",
      "best n_neighbors: 1\n",
      "test-set score: 0.974\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the data \n",
    "X, y = load_iris(return_X_y = True)\n",
    "\n",
    "# Split of the test set \n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=55)\n",
    "\n",
    "# Split trainval into train + val \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.25, random_state=5)\n",
    "\n",
    "# create a list to hold the perf. results on validation set \n",
    "val_scores = [] \n",
    "# specify hyper-parameter values \n",
    "nbrs = np.arange(1,16,2)\n",
    "\n",
    "for n in nbrs: \n",
    "    # build a model \n",
    "    knn = KNeighborsClassifier(n_neighbors=n)\n",
    "    knn.fit(X_train, y_train)\n",
    "    # calculate performance on validation set \n",
    "    val_scores.append(knn.score(X_val, y_val))\n",
    "\n",
    "# Find the best score and best hyper-parameter \n",
    "print(\"best validation score:  %.3f\" % np.max(val_scores))\n",
    "best_nbrs = nbrs[np.argmax(val_scores)]\n",
    "print(\"best n_neighbors: %d\" % best_nbrs)\n",
    "\n",
    "# Retrain model on train + validation set \n",
    "knn = KNeighborsClassifier(n_neighbors=best_nbrs)\n",
    "knn.fit(X_trainval, y_trainval)\n",
    "print(\"test-set score: %.3f\" % knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kh-XXLEv55Br"
   },
   "source": [
    "This approach has improved upon the hold-out set method, but still relies on the particular splits.  What is we change the random splits, we might end up with different results.  In fact, if we see different outcomes based on our splits, it may mean the model is not very robust or there is not enough data. \n",
    "\n",
    "#### *How can we make it more robust?*  \n",
    "\n",
    "**A** Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gc-HIxN36fTP"
   },
   "source": [
    "## 3 - K-fold cross-validation \n",
    "\n",
    "The basic premise of cross-validation is to replace the split into training and validation data with multiple different splits.  Most commonly, cross validation is applied to the training/validation split, but it can also be applied to splitting off the test data. \n",
    "\n",
    "<img src=\"https://pages.mtu.edu/~lebrown/un5550-f21/p6/cross_validation_new.png\" width=\"50%\">\n",
    "\n",
    "The most common variant of cross-validation is k-fold cross validation, the image above illustrates a 5-fold cross-validation.  \n",
    "\n",
    "For each fold, a split of the data is made where this fold is the validation data, and the rest is the training data.  For the 5-fold cross-validation, we split the data into five parts, and have 5 different training/validation splits.  We build a model for each of the splits using the training part and validation part to evaluate it.  The outcome is five different performance values.  These can be aggregated - compute a mean/median, or use them to estimate a variance over the splits.  \n",
    "\n",
    "This approach is more robust over using a single split.  All of the initial train/validation data is used in the validation set exactly once, where a single split only some of the data appears in the validation set.   The main disadvantage of cross-validation is the computational cost.  \n",
    "\n",
    "Another issue of k-fold cross-validation is that it doesn't produce a model, it produced k models.  If you want to make predictions on new data, how to do so?  One obvious method is to retrain on the whole train/validation set. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6KiVNld2MBu"
   },
   "source": [
    "We can do cross-validation by hand, i.e., using the `KFold()` family of methods.  Alternatively, we can use the cross-validation functions: `cross_val_score` and `cross_validate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w2h3K6Nx2K2u",
    "outputId": "7a371cad-52e1-41e2-cfc0-080c1d7256b4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean validation score:  0.973\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold \n",
    "\n",
    "# Perform cross-validation by hand \n",
    "\n",
    "# Split of the test set \n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=12) \n",
    " \n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=124)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for tr_indx, val_indx in kf.split(X_trainval, y_trainval):\n",
    "    X_train, X_val = X_trainval[tr_indx], X_trainval[val_indx]\n",
    "    y_train, y_val = y_trainval[tr_indx], y_trainval[val_indx]\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_val)\n",
    "    scores.append(metrics.accuracy_score(y_val, y_pred))\n",
    "\n",
    "print(\"mean validation score:  %.3f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGsHknrL_6j_"
   },
   "source": [
    "## 4 - Grid Search with Cross-validation in Validation Split\n",
    "\n",
    "Let's now think about doing model selection, but using cross-validation rather than a single split.  \n",
    "\n",
    "The overall idea is illustrated below. We still have the initial split into training and test data.  But rather than a single split into training and validation data, we run cross-validation for each parameter setting.  We record the mean score averaged over the splits in the cross-validation.  \n",
    "\n",
    "After evaluating all candidate paramters, find the one with the best mean performance.  *Keep in mind this score does not correspond to a single model; there is no best model*.  \n",
    "\n",
    "We select the hyper-parameters that are best on average over the splits.  Then we build a new model, using the hyper-parameters that performed best on average in cross-validation, on the full training dataset (X_trainval).  Finally, we evaluate this model on the test data set.   \n",
    "\n",
    "\n",
    "<img src=\"https://pages.mtu.edu/~lebrown/un5550-f21/p6/grid_search_cross_validation_new.png\" width=\"60%\">\n",
    "\n",
    "<img src=\"https://pages.mtu.edu/~lebrown/un5550-f21/p6/cv.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDw8rWqn_R8e",
    "outputId": "b647eab2-986f-4b12-b024-32a8012fa79d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross-validation score: 0.975\n",
      "best n_neighbors: 15\n",
      "test-set score: 0.967\n"
     ]
    }
   ],
   "source": [
    "# Split of the test set \n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=55)\n",
    "\n",
    "# create a list to hold the perf. results on validation sets \n",
    "cross_val_scores = [] \n",
    "# specify hyper-parameter values \n",
    "nbrs = np.arange(1,16,2)\n",
    "\n",
    "for n in nbrs: \n",
    "    # build the model with hyper-parameters \n",
    "    knn = KNeighborsClassifier(n_neighbors=n)\n",
    "    # Instead of fitting a single model, we perform cross-validation \n",
    "    scores = cross_val_score(knn, X_trainval, y_trainval, cv=10)\n",
    "    # record the average over the 10 folds \n",
    "    cross_val_scores.append(np.mean(scores))\n",
    "\n",
    "print(f\"best cross-validation score: {np.max(cross_val_scores):.3}\")\n",
    "best_nbrs = nbrs[np.argmax(cross_val_scores)]\n",
    "print(f\"best n_neighbors: {best_nbrs}\")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=best_nbrs)\n",
    "knn.fit(X_trainval, y_trainval)\n",
    "print(f\"test-set score: {knn.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEgPU_G4CTZU"
   },
   "source": [
    "The code above of grid-search with cross-validation and a hold-out test set is a gold standard approach for model comparison and parameter tuning.  \n",
    "\n",
    "**ASIDE: Cross-validation vs. Grid Search** \n",
    "\n",
    "Students often conflate the use of cross-validation with the use of grid search.  These are distinct and should not be used interchangeably.  **Cross-validation** is a technique to robustly evaluate a particular model on a particular data set.  **Grid search** is a technique to tune the hyper-parameters of a particular model by brute-force search.  Often each candidate is evaluated using cross-validaiton, but it is not necessary (you could use a single split of training + validation set).  So while cross-validation is often used within a grid search, you can also do cross-validation outside of a grid search, and you can do a grid search without using cross-validation.\n",
    "\n",
    "The overall approach is illustrated below.  Start by specifying hyper-parameters to evaluate (generally this means selecting the models we are using as well).  Split the data into training and test sets.  For each hyper-parameter candidate, run a grid search on the training set, yielding a score for each split, and a mean score over all splits.  The mean validation scores are used to select the best hyper-parameter value and retrain a model on the whole training data.  Then we evaluate this final model on the test set. \n",
    "\n",
    "\n",
    "<img src=\"https://pages.mtu.edu/~lebrown/un5550-f21/p6/gridsearch_workflow.png\" width=\"60%\">\n",
    "Image from scikit-learn.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "This pattern of evaluation is common, therefore, `scikit-learn` has a method `GridSearchCV`, which does most of this for you. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fz-Qkjb2E_ly"
   },
   "source": [
    "## 5 - GridSearchCV \n",
    "\n",
    "The `GridSearchCV` class is a meta-estimator, it takes any scikit-learn model and tunes the hyper-parameters for you using cross-validation.  The hyper-parameter grid is specified as a dictionary where the keys are the names of the parameters in the estimator and the values are all the candidate values of the hyper-parameter we want to evaluate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEdC9f5HFcqC",
    "outputId": "5bdbf801-9ed5-41b2-f241-4f6fc4076163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.975\n",
      "best parameters: {'n_neighbors': 15}\n",
      "test-set score: 0.967\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split of the test set \n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=55)\n",
    "\n",
    "# define the parameter grid \n",
    "param_grid = {'n_neighbors': np.arange(1, 16, 2)}\n",
    "\n",
    "# Instantiate GridSearchCV - sets up the parameters on how to run \n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=10, \n",
    "                    return_train_score=True)\n",
    "# Execute the search (and retrain the final model) \n",
    "grid.fit(X_trainval, y_trainval)\n",
    "\n",
    "print(f\"best mean cross-validation score: {grid.best_score_}\")\n",
    "print(f\"best parameters: {grid.best_params_}\")\n",
    "\n",
    "# do a final evaluation on the test set \n",
    "print(f\"test-set score: {grid.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDGwAQpBGQnp"
   },
   "source": [
    "Because grid search is a meta-estimator, after you instantiate it, you can use it like any other scikit-learn model: use `fit`, `predict`, `score` methods using the best hyper-parameter setting. \n",
    "\n",
    "The test set is reserved for the final evaluation, therefore, it can be a good idea to look at the search results without the test set.  If the `best_score_` is lower than expected or needed for an application, do not use the test set.  Also, you may want to look at whether the `best_params_` value is on the boundary of the search space specified.  If it is, you may want to extend the range.  Also, the model that was refit on the whole training + validation data (the model used when calling `predict` and `score`) is called as `best_estimator_`.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lO6iCn3QIkbn"
   },
   "source": [
    "## 6 - Nested Cross-Validation \n",
    "\n",
    "As mentioned above, it is most common to use cross-validation in the training/validation part of a three-fold split: train/validationa/test split.  However, it can be used for both, resulting in **nested cross-validation**.  Nested cross-validation is easy to implement, but not commonly used for three reasons:\n",
    "\n",
    "* computationally expensive, adds another loop \n",
    "* it doesn't result in a single model, so it's hard to productionize \n",
    "* it is harder to understand \n",
    "\n",
    "Here we can see an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_AdcbraMJc0K",
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors':  np.arange(1, 15, 2)}\n",
    "# instantiate grid search\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=10,\n",
    "                   return_train_score=True)\n",
    "# perform cross-validation on the grid-search estimator\n",
    "# where each individual fit will internally perform cross-validation\n",
    "res = cross_validate(grid, X, y, cv=5, return_train_score=True, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "OhP3qd8hJ9xL",
    "outputId": "15857056-ec01-412d-f1a4-98a509a80216",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>estimator</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.175930</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>GridSearchCV(cv=10, estimator=KNeighborsClassi...</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.175057</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>GridSearchCV(cv=10, estimator=KNeighborsClassi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.172843</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>GridSearchCV(cv=10, estimator=KNeighborsClassi...</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.171521</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>GridSearchCV(cv=10, estimator=KNeighborsClassi...</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>GridSearchCV(cv=10, estimator=KNeighborsClassi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time                                          estimator  \\\n",
       "0  0.175930    0.000785  GridSearchCV(cv=10, estimator=KNeighborsClassi...   \n",
       "1  0.175057    0.000886  GridSearchCV(cv=10, estimator=KNeighborsClassi...   \n",
       "2  0.172843    0.000773  GridSearchCV(cv=10, estimator=KNeighborsClassi...   \n",
       "3  0.171521    0.000772  GridSearchCV(cv=10, estimator=KNeighborsClassi...   \n",
       "4  0.170900    0.000782  GridSearchCV(cv=10, estimator=KNeighborsClassi...   \n",
       "\n",
       "   test_score  train_score  \n",
       "0    0.966667     0.975000  \n",
       "1    1.000000     0.966667  \n",
       "2    0.933333     0.966667  \n",
       "3    0.966667     0.983333  \n",
       "4    1.000000     0.966667  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCFvTQRUJ7SH"
   },
   "source": [
    "Before, we had 8 hyper-parameter values (odds between 1-15), and 10 cross-validation folds, and a final evaluation model, so 81 models were learned.  With the outer cross-validation loop, there are 405 models, which adds time. \n",
    "\n",
    "Also, the outcome is five different scores, for each split.  However, these don't match to a single model, because the grid search may lead to different optimum parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0x5tLwm-KrL8",
    "outputId": "2fe17bb1-2bae-4ab1-fa5a-04445c07dd61",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'n_neighbors': 9},\n",
       " {'n_neighbors': 7},\n",
       " {'n_neighbors': 3},\n",
       " {'n_neighbors': 7},\n",
       " {'n_neighbors': 11}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.best_params_ for x in res['estimator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDKGTySBL_gj"
   },
   "source": [
    "In this case, there is not a model that can be immediately used on new data.  \n",
    "\n",
    "Because of these reasons we are instead going to use the **best practice is described in sections 4 and 5** above to select hyper-parameters and evaluate the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU8WG7xmROXI"
   },
   "source": [
    "# Data Leakage \n",
    "\n",
    "*Copied and adpated from MLmastery Data Leakage page.*\n",
    "\n",
    "A very common error when using cross-validation is data leakage. Data leakage is where information about the holdout dataset, such as a test or validation dataset, is made available to the model in the training dataset.   This is not a direct type of data leakage, where we would train the model on the test dataset. Instead, it is an indirect type of data leakage, where some knowledge about the test dataset, captured in summary statistics is available to the model during training. This can make it a harder type of data leakage to spot, especially for beginners.\n",
    "\n",
    "For example, consider a problem where we want to standardize the data, that is, scale the data so that each variables has a mean of 0 and standard deviation of 1.  When we standardize the input variables, this requires that we first calculate the mean and standard deviation values for each variable before using these values to scale the variables. The dataset is then split into train and test datasets, but the examples in the training dataset know something about the data in the test dataset; they have been scaled by the mean and standard deviation values, so they know more about the global distribution of the variable then they should.\n",
    "\n",
    "This type of data leakage exists with almost any data preparation task, e.g., standardization or even imputation of missing values.  \n",
    "\n",
    "#### *How to solve this issue?* \n",
    "\n",
    "Data preparation must be fit on the training data set only.  More generally, the entire modeling pipeline must be prepared only on the training dataset to avoid data leakage. This might include data transforms, but also other techniques such feature selection, dimensionality reduction, feature engineering and more. \n",
    "\n",
    "Let's see an example of this issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UN9DxpbIToah"
   },
   "source": [
    "## Example of Data Leakage on Hold-out set\n",
    "\n",
    "We will start with some synthetic data for a binary classification problem. \n",
    "\n",
    "\n",
    "#### BAD EXAMPLE \n",
    "\n",
    "The naive approach for scaling the data is:    \n",
    "\n",
    "1. Run scaling on the entire data set \n",
    "2. Split the data into train/test \n",
    "3. Train the model on train, evaluate on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7mL8N8iT9or",
    "outputId": "8a7c01d9-aa57-44cc-e0e5-faeec9a4687f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.800\n"
     ]
    }
   ],
   "source": [
    "# BAD - EXAMPLE OF DATA LEAKAGE\n",
    "\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn import metrics\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=13, \n",
    "                           n_redundant=7, random_state=20)\n",
    "\n",
    "# normalize the dataset, scale values to be between 0-1\n",
    "scaler = MinMaxScaler()\n",
    "Xsc = scaler.fit_transform(X)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    Xsc, y, test_size=0.25, random_state=5)\n",
    "\n",
    "# fit the model\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = metrics.accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLFs0qPcUexw"
   },
   "source": [
    "#### GOOD - FIXED DATA LEAKAGE\n",
    "\n",
    "Let's look at how we should do the data preparation to avoid data leakage:    \n",
    "\n",
    "1. Split the data into train/test \n",
    "2. Run scaling, use train to set parameters, apply to both train and test \n",
    "3. Train the model on train, evaluate on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KciT3lAqUwPa",
    "outputId": "8d85cabb-3ea2-4033-ee4c-500c147a445a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.400\n"
     ]
    }
   ],
   "source": [
    "# GOOD - FIXED DATA LEAKAGE\n",
    "\n",
    "# split into train and test sets\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=5)\n",
    "\n",
    "# define the scaler\n",
    "scaler = MinMaxScaler()\n",
    "# fit on the training dataset\n",
    "scaler.fit(X_trainval)\n",
    "# scale the training dataset\n",
    "X_trainval_scaled = scaler.transform(X_trainval)\n",
    "# scale the test dataset\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# fit the model\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_trainval_scaled, y_trainval)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_scaled)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = metrics.accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9FT01GcV8-w"
   },
   "source": [
    "The model with data leakage has slightly better performance that that without. *Note, this may change across random splits*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KebYcHYyW4lW"
   },
   "source": [
    "## Example of Data Leakage in Cross Validation \n",
    "\n",
    "#### BAD EXAMPLE\n",
    "\n",
    "Naive data preparation with cross-validation involves applying the data transform first, then using the cross-validation procedure. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8oL0HQbeXLJd",
    "outputId": "bda02de2-2a90-424c-b7d7-c01dc12bad3e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.800 (2.344)\n"
     ]
    }
   ],
   "source": [
    "# BAD - EXAMPLE OF DATA LEAKAGE\n",
    "\n",
    "# from sklearn import model_selection\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler()\n",
    "Xsc = scaler.fit_transform(X)\n",
    "\n",
    "# define the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# define the evaluation procedure\n",
    "cv = model_selection.RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate the model using cross-validation\n",
    "scores = cross_val_score(model, Xsc, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(scores)*100, np.std(scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dSs_2mvX5SE"
   },
   "source": [
    "#### GOOD - FIXED DATA LEAKAGE\n",
    "\n",
    "Let's look at the correct way to do data preparation with cross-validation.  \n",
    "\n",
    "It requires that the data preparation method is prepared on the training set and applied to the train and test sets within the cross-validation procedure.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mv-SH7TXdBYC",
    "outputId": "067871ec-2f1a-4362-d56e-41cd7ec7360a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.767 (2.376)\n"
     ]
    }
   ],
   "source": [
    "# GOOD - FIXED DATA LEAKAGE\n",
    "\n",
    "# Set up how to perform k-fold \n",
    "kf = model_selection.RepeatedStratifiedKFold(n_splits = 10, \n",
    "                                             n_repeats=3, random_state=1)\n",
    "scores = [] \n",
    "\n",
    "# Loop over splits\n",
    "for tr_indx, te_indx in kf.split(X, y): \n",
    "    X_train, X_test = X[tr_indx], X[te_indx]\n",
    "    y_train, y_test = y[tr_indx], y[te_indx]\n",
    "    \n",
    "    # normalize the dataset\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # define the model\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(X_train_scaled, y_train) \n",
    "    yhat = model.predict(X_test_scaled)\n",
    "\n",
    "    scores.append(metrics.accuracy_score(y_test, yhat))\n",
    "\n",
    "#print(scores)\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(scores)*100, np.std(scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqvsS6Y3izAk"
   },
   "source": [
    "## Example of Data Leakage with GridSearchCV \n",
    "\n",
    "#### BAD EXAMPLE \n",
    "\n",
    "Let's look at how we get data leakage when using GridSearchCV as discussed above. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGmnIQP-jDCU",
    "outputId": "56155562-4600-48f2-ac38-befd2aaaaeb0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.9333333333333333\n",
      "best parameters: {'n_neighbors': 3}\n"
     ]
    }
   ],
   "source": [
    "# BAD - EXAMPLE OF DATA LEAKAGE\n",
    "\n",
    "# Split of the test set \n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=5)\n",
    "\n",
    "# Scale the data \n",
    "scaler = MinMaxScaler()\n",
    "X_trainval_sc = scaler.fit_transform(X_trainval)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate the model \n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# params for Grid Search \n",
    "params = {'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15]}\n",
    "\n",
    "# Use GridSearchCV \n",
    "grid = GridSearchCV(knn, params, cv=5, return_train_score=True)\n",
    "grid.fit(X_trainval_sc, y_trainval) \n",
    "\n",
    "print(f\"best mean cross-validation score: {grid.best_score_}\")\n",
    "print(f\"best parameters: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4-_IepVj-Fx"
   },
   "source": [
    "##### *What's the problem?* \n",
    "\n",
    "The scaling uses the data in train+validation to set the parameters and apply the scaling to the test set. \n",
    "\n",
    "The issue is with the GridSearchCV usage.  GridSearchCV will split the train+validation dataset into the train set and a validation set. See the image below from scikit-learn to illustrate this idea again. \n",
    "\n",
    "<img src=\"https://pages.mtu.edu/~lebrown/un5550-f21/p6/gridsearchSV.png\" width=\"60%\">\n",
    "Image from scikit-learn. \n",
    "\n",
    "<br><br> \n",
    "\n",
    "Within the cross-validation, the validation set should be treated as a temporary unseen test set.  Therefore, the scaler should not be fit using this data. \n",
    "\n",
    "#### *How do we solve data leakage in this case?*\n",
    "\n",
    "Use **pipelines**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GR-2gQr7vVDO"
   },
   "source": [
    "## Example of a Pipeline \n",
    "\n",
    "[Pipelines](https://scikit-learn.org/stable/data_transforms.html) allow us to use a number of different dataset transformations, we may clean, preprocess, reduce, or create feature representations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdqS2QqmxMaa"
   },
   "source": [
    "### Pipeline on Hold-out set \n",
    "\n",
    "Let's see an example of scaling the data using a pipeline.  From above, we have an example of data leakage. \n",
    "\n",
    "```python\n",
    "# BAD - Example of Data Leakage\n",
    "\n",
    "# normalize the dataset \n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=5)\n",
    "# fit the model\n",
    "model = KNeighborsClassifier().fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "accuracy = metrics.accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))\n",
    "```\n",
    "\n",
    "Here, was the corrected code to prevent data leakage and compare it to a pipeline. \n",
    "\n",
    "```python \n",
    "# GOOD - Example without Data Leakage \n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=5)\n",
    "# define the scaler\n",
    "scaler = MinMaxScaler()\n",
    "# fit on the training dataset\n",
    "scaler.fit(X_train)\n",
    "# scale the training dataset\n",
    "X_train = scaler.transform(X_train)\n",
    "# scale the test dataset\n",
    "X_test = scaler.transform(X_test)\n",
    "# fit the model\n",
    "model = KNeighborsClassifier().fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "accuracy = metrics.accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))\n",
    "```\n",
    "\n",
    "Now below is this example implemented as a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lmqhNyGwTwU",
    "outputId": "27bce91f-b297-426d-9002-bf3e07206621",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.400\n"
     ]
    }
   ],
   "source": [
    "# Pipeline to avoid data leakage\n",
    "\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=5)\n",
    "\n",
    "# Setup the pipeline \n",
    "pipe = make_pipeline(MinMaxScaler(), KNeighborsClassifier())\n",
    "\n",
    "# Execute the pipeline with the data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "res = pipe.score(X_test, y_test)\n",
    "\n",
    "# evaluate predictions\n",
    "print('Accuracy: %.3f' % (res*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D73dQkLSxZ1f"
   },
   "source": [
    "### Pipeline with Cross-validation \n",
    "\n",
    "In the example below, we can see how cross-validation with data leakage can be converted to a pipeline and also how we can eliminate data leakage when using for-loops for cross-validation and in a pipeline. \n",
    "\n",
    "You will want to use the code on the right (\"GOOD\") in the future. \n",
    "\n",
    "#### Preprocessing before cross-validation *BAD - DO NOT USE*\n",
    "\n",
    "\n",
    "```python\n",
    "# BAD!\n",
    "scaler = MinMaxScaler()\n",
    "X_sc = scaler.fit_transform(X)\n",
    "\n",
    "scores = []\n",
    "for tr_indx, te_indx in KFold().split(X_sc, Y):\n",
    "    knn = KNeighborsClassifier().fit(X_sc[train], y[train])\n",
    "    score = knn.score(X_sc[test], y[test])\n",
    "    scores.append(score)\n",
    "```\n",
    "\n",
    "Which is equivalent to the following condensed code: \n",
    "\n",
    "```python\n",
    "scaler = MinMaxScaler()\n",
    "X_sc = scalar.fit_transform(X)\n",
    "scores = cross_val_score(KNeighborsClassifier(), X_sc, y)\n",
    "```\n",
    "\n",
    "#### Preprocessing within cross validation  *GOOD - USE as EXAMPLE*\n",
    "\n",
    "\n",
    "```python\n",
    "# GOOD!\n",
    "scores = []\n",
    "scaler = MinMaxScaler()\n",
    "for train, test in KFold().split(X, y):\n",
    "    scaler.fit(X[train], y[train])\n",
    "    X_sc_train = scaler.transform(X[train])\n",
    "    knn = KNeighborsClassifier().fit(X_sc_train, y[train])\n",
    "    X_sc_test = scaler.transform(X[test])\n",
    "    score = knn.score(X_sc_test, y[test])\n",
    "    scores.append(score)\n",
    "```\n",
    "\n",
    "Which is equivalent to: \n",
    "\n",
    "```python\n",
    "pipe = make_pipeline(MinMaxScaler(), KNeighborsClassifier())\n",
    "scores = cross_val_score(pipe, X, y)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXQq-Icg3ml5",
    "outputId": "35a72078-4b31-4fad-ca14-c60fa8d479b8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Acc: 94.700\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipe = make_pipeline(MinMaxScaler(), KNeighborsClassifier())\n",
    "scores = cross_val_score(pipe, X, y)\n",
    "\n",
    "print(\"Mean Acc: %.3f\" % (np.mean(scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhqZzP51363U"
   },
   "source": [
    "### Pipeline with GridSearchCV \n",
    "\n",
    "If you recall, `GridSearchCV` is passed an estimator and a dictionary of parameter values for tuning hyper-parameters.  We can pass a `Pipeline` as the estimator, but we need to adjust the process above to ensure the parameter tuning is applied to the correct step of the pipeline.  This is done by specifying the hyper-parameters within a pipeline, by using the name of the step of the pipeline, followed by the double underscore ('dunder'), followed by the name of the hyper-parameter. \n",
    "\n",
    "So, when we create a pipeline and we want to tune the `n_neighbors` parameter of `KNeighborsClassifier`, we need to use  `kneighborsclassifier__n_neighbors` as the hyper-parameter name. \n",
    "Below is the example code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7IJNYXFr5DJY",
    "outputId": "0a1457a7-d009-4491-b7a8-971ab1341ea1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kneighborsclassifier__n_neighbors': 5}\n",
      "94.39999999999999\n"
     ]
    }
   ],
   "source": [
    "# GOOD - EXAMPLE without Data Leakage using pipeline and GridSearchCV\n",
    "\n",
    "# Split of the test set \n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=5)\n",
    "\n",
    "# create the pipeline \n",
    "knn_pipe = make_pipeline(MinMaxScaler(), KNeighborsClassifier())\n",
    "\n",
    "# create the parameter grid \n",
    "# Pipeline hyper-parameters are specified as <step name>__<hyper-parameter name>\n",
    "params = {'kneighborsclassifier__n_neighbors': \n",
    "          [1, 3, 5, 7, 9, 11, 13, 15]}\n",
    "\n",
    "# Setup cross-validation for repeatability \n",
    "cvStrat = StratifiedKFold(n_splits=10, random_state=5, shuffle=True)\n",
    "\n",
    "# Instantiate the grid-search\n",
    "grid = GridSearchCV(knn_pipe, params, cv=cvStrat)\n",
    "# run the grid search and report results \n",
    "grid.fit(X_trainval, y_trainval)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2L5w8OTD6VQ2"
   },
   "source": [
    "We can look at all the parameters with:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35_WS_Me6aRc",
    "outputId": "4fd3a09f-f1be-454e-81eb-5e714d8f1f9e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('minmaxscaler', MinMaxScaler()),\n",
       "  ('kneighborsclassifier', KNeighborsClassifier())],\n",
       " 'verbose': False,\n",
       " 'minmaxscaler': MinMaxScaler(),\n",
       " 'kneighborsclassifier': KNeighborsClassifier(),\n",
       " 'minmaxscaler__clip': False,\n",
       " 'minmaxscaler__copy': True,\n",
       " 'minmaxscaler__feature_range': (0, 1),\n",
       " 'kneighborsclassifier__algorithm': 'auto',\n",
       " 'kneighborsclassifier__leaf_size': 30,\n",
       " 'kneighborsclassifier__metric': 'minkowski',\n",
       " 'kneighborsclassifier__metric_params': None,\n",
       " 'kneighborsclassifier__n_jobs': None,\n",
       " 'kneighborsclassifier__n_neighbors': 5,\n",
       " 'kneighborsclassifier__p': 2,\n",
       " 'kneighborsclassifier__weights': 'uniform'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYlRSHT26s4u"
   },
   "source": [
    "We could also tune parameters of the pre-processing step.  Here we are adding a feature selection step to choose only a percentage of the top features to be included in the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjAiBOEs6yT6",
    "outputId": "1e201910-4486-4c50-fff8-4c5dd18dc2f2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kneighborsclassifier__n_neighbors': 3, 'selectpercentile__percentile': 100}\n",
      "94.0\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "# create a pipeline\n",
    "select_pipe = make_pipeline(MinMaxScaler(), SelectPercentile(), \n",
    "                            KNeighborsClassifier())\n",
    "\n",
    "# create the search grid.\n",
    "# Pipeline hyper-parameters are specified as <step name>__<hyper-parameter name>\n",
    "param_grid = {'kneighborsclassifier__n_neighbors': [1, 3, 5, 7, 9, 10, 13, 15],\n",
    "              'selectpercentile__percentile': [1, 2, 5, 10, 50, 100]}\n",
    "\n",
    "# Instantiate grid-search, here we use default 10-fold cross-validation\n",
    "grid = GridSearchCV(select_pipe, param_grid, cv=10)\n",
    "\n",
    "# run the grid-search and report results\n",
    "grid.fit(X_trainval, y_trainval)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZY_Wl9s7jVD"
   },
   "source": [
    "Note, we can make the parameter names of the GridSearch a bit simpler by using establishing some abbreviations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hRhGHWJT7sjH",
    "outputId": "be50d5af-1aff-411c-c4bc-7e694255c8d9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fs__percentile': 100, 'knn__n_neighbors': 3}\n",
      "94.0\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create a pipeline\n",
    "#  Label each step of the pipeline with a name, e.g., \n",
    "#   'sc' - for scaling \n",
    "#   'fs' - for Feature Selection\n",
    "#   'knn' - for KNN classifier \n",
    "select_pipe = Pipeline([\n",
    "                        ('sc', MinMaxScaler()),\n",
    "                        ('fs', SelectPercentile()),\n",
    "                        ('knn', KNeighborsClassifier())])\n",
    "\n",
    "# create the search grid.\n",
    "# Pipeline hyper-parameters are specified as <step name>__<hyper-parameter name>\n",
    "param_grid = {'knn__n_neighbors': [1, 3, 5, 7, 9, 10, 13, 15],\n",
    "              'fs__percentile': [1, 2, 5, 10, 50, 100]}\n",
    "\n",
    "# Instantiate grid-search\n",
    "grid = GridSearchCV(select_pipe, param_grid, cv=10)\n",
    "\n",
    "# run the grid-search and report results\n",
    "grid.fit(X_trainval, y_trainval)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fzsp91JE-HWw"
   },
   "source": [
    "### Different options in a Pipeline. \n",
    "\n",
    "We may want the pipeline to select what preprocessing steps to include or what models to apply.  For example, I have been using `MinMaxScaler` in the examples, but what if instead we should use `StandardScaler` for this dataset.  We can let `GridSearchCV` answer this. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uwr5m0JV-gMn",
    "outputId": "34fe35a8-5fe1-4fef-be9b-1d99b465ba0c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'knn__n_neighbors': 5, 'scaler': StandardScaler()}\n",
      "95.19999999999999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# declare a two step pipeline, explicitly giving names to both steps.\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('knn', KNeighborsClassifier())])\n",
    "\n",
    "# The name of the first step is 'scaler' and we can assign different\n",
    "# estimators to this step, such as MinMaxScaler or StandardScaler\n",
    "# There is a special value 'passthrough' which skips the step\n",
    "param_grid = {'scaler': [MinMaxScaler(), StandardScaler(), 'passthrough'],\n",
    "              # we named the second step knn, so we have to use that name here\n",
    "              'knn__n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15]}\n",
    "\n",
    "# instantiate and run as before:\n",
    "grid = GridSearchCV(pipe, param_grid, cv=10)\n",
    "grid.fit(X_trainval, y_trainval)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4wrknDy--e-"
   },
   "source": [
    "Remember, we can see the detailed results with `cv_results_` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ty2JCJ4U_DUp",
    "outputId": "52c0fae6-dce8-4752-c6cf-7c865247c5aa",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00041046, 0.00047514, 0.0002316 , 0.00037699, 0.00045443,\n",
       "        0.00023468, 0.00037916, 0.00046697, 0.00023632, 0.00038772,\n",
       "        0.00046046, 0.000226  , 0.00038333, 0.00046885, 0.00023236,\n",
       "        0.0003706 , 0.0004684 , 0.00022676, 0.00037739, 0.00046148,\n",
       "        0.00022328, 0.00038228, 0.00047467, 0.00023615]),\n",
       " 'std_fit_time': array([3.76494870e-05, 4.64215668e-05, 1.63177271e-05, 9.05978052e-06,\n",
       "        1.22046096e-05, 1.38340519e-05, 2.10929848e-05, 3.91775050e-05,\n",
       "        2.17940665e-05, 4.08809379e-05, 2.17099853e-05, 1.81444058e-05,\n",
       "        2.15108002e-05, 2.62325994e-05, 2.22362107e-05, 9.32347735e-06,\n",
       "        1.42131072e-05, 3.32155674e-06, 7.83265814e-06, 1.60734443e-05,\n",
       "        7.98631067e-06, 1.45220989e-05, 5.31514587e-05, 1.19729101e-05]),\n",
       " 'mean_score_time': array([0.00213363, 0.00193734, 0.00174739, 0.00162449, 0.00162168,\n",
       "        0.0017905 , 0.00152376, 0.0014066 , 0.00177784, 0.00174584,\n",
       "        0.00148649, 0.00145545, 0.00159638, 0.00186791, 0.00144908,\n",
       "        0.00143926, 0.00164337, 0.00148191, 0.00159366, 0.00152986,\n",
       "        0.0015332 , 0.00161686, 0.00184262, 0.00210221]),\n",
       " 'std_score_time': array([1.12341960e-03, 1.06527703e-03, 5.10660155e-04, 1.75482851e-04,\n",
       "        4.89584304e-04, 9.20296206e-04, 2.52832206e-04, 3.76396493e-05,\n",
       "        6.94904631e-04, 6.22723060e-04, 2.02613007e-04, 2.28401694e-04,\n",
       "        3.17427206e-04, 7.02494630e-04, 7.63502844e-05, 4.77573363e-05,\n",
       "        1.68243726e-04, 7.58967042e-05, 2.60081211e-04, 1.31741704e-04,\n",
       "        2.56485784e-04, 2.74777763e-04, 8.51055537e-04, 9.35828840e-04]),\n",
       " 'param_knn__n_neighbors': masked_array(data=[1, 1, 1, 3, 3, 3, 5, 5, 5, 7, 7, 7, 9, 9, 9, 11, 11,\n",
       "                    11, 13, 13, 13, 15, 15, 15],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_scaler': masked_array(data=[MinMaxScaler(), StandardScaler(), 'passthrough',\n",
       "                    MinMaxScaler(), StandardScaler(), 'passthrough',\n",
       "                    MinMaxScaler(), StandardScaler(), 'passthrough',\n",
       "                    MinMaxScaler(), StandardScaler(), 'passthrough',\n",
       "                    MinMaxScaler(), StandardScaler(), 'passthrough',\n",
       "                    MinMaxScaler(), StandardScaler(), 'passthrough',\n",
       "                    MinMaxScaler(), StandardScaler(), 'passthrough',\n",
       "                    MinMaxScaler(), StandardScaler(), 'passthrough'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'knn__n_neighbors': 1, 'scaler': MinMaxScaler()},\n",
       "  {'knn__n_neighbors': 1, 'scaler': StandardScaler()},\n",
       "  {'knn__n_neighbors': 1, 'scaler': 'passthrough'},\n",
       "  {'knn__n_neighbors': 3, 'scaler': MinMaxScaler()},\n",
       "  {'knn__n_neighbors': 3, 'scaler': StandardScaler()},\n",
       "  {'knn__n_neighbors': 3, 'scaler': 'passthrough'},\n",
       "  {'knn__n_neighbors': 5, 'scaler': MinMaxScaler()},\n",
       "  {'knn__n_neighbors': 5, 'scaler': StandardScaler()},\n",
       "  {'knn__n_neighbors': 5, 'scaler': 'passthrough'},\n",
       "  {'knn__n_neighbors': 7, 'scaler': MinMaxScaler()},\n",
       "  {'knn__n_neighbors': 7, 'scaler': StandardScaler()},\n",
       "  {'knn__n_neighbors': 7, 'scaler': 'passthrough'},\n",
       "  {'knn__n_neighbors': 9, 'scaler': MinMaxScaler()},\n",
       "  {'knn__n_neighbors': 9, 'scaler': StandardScaler()},\n",
       "  {'knn__n_neighbors': 9, 'scaler': 'passthrough'},\n",
       "  {'knn__n_neighbors': 11, 'scaler': MinMaxScaler()},\n",
       "  {'knn__n_neighbors': 11, 'scaler': StandardScaler()},\n",
       "  {'knn__n_neighbors': 11, 'scaler': 'passthrough'},\n",
       "  {'knn__n_neighbors': 13, 'scaler': MinMaxScaler()},\n",
       "  {'knn__n_neighbors': 13, 'scaler': StandardScaler()},\n",
       "  {'knn__n_neighbors': 13, 'scaler': 'passthrough'},\n",
       "  {'knn__n_neighbors': 15, 'scaler': MinMaxScaler()},\n",
       "  {'knn__n_neighbors': 15, 'scaler': StandardScaler()},\n",
       "  {'knn__n_neighbors': 15, 'scaler': 'passthrough'}],\n",
       " 'split0_test_score': array([0.94666667, 0.94666667, 0.88      , 0.94666667, 0.94666667,\n",
       "        0.89333333, 0.97333333, 0.97333333, 0.90666667, 0.98666667,\n",
       "        0.97333333, 0.92      , 0.97333333, 0.97333333, 0.88      ,\n",
       "        0.97333333, 0.97333333, 0.88      , 0.97333333, 0.97333333,\n",
       "        0.88      , 0.97333333, 0.93333333, 0.85333333]),\n",
       " 'split1_test_score': array([0.97333333, 0.97333333, 0.92      , 0.98666667, 0.98666667,\n",
       "        0.93333333, 0.96      , 0.96      , 0.93333333, 0.92      ,\n",
       "        0.94666667, 0.93333333, 0.92      , 0.93333333, 0.90666667,\n",
       "        0.93333333, 0.93333333, 0.90666667, 0.93333333, 0.94666667,\n",
       "        0.92      , 0.90666667, 0.93333333, 0.90666667]),\n",
       " 'split2_test_score': array([0.88      , 0.88      , 0.86666667, 0.90666667, 0.90666667,\n",
       "        0.92      , 0.93333333, 0.94666667, 0.92      , 0.96      ,\n",
       "        0.94666667, 0.92      , 0.97333333, 0.96      , 0.89333333,\n",
       "        0.97333333, 0.97333333, 0.92      , 0.97333333, 0.96      ,\n",
       "        0.92      , 0.96      , 0.97333333, 0.90666667]),\n",
       " 'split3_test_score': array([0.89333333, 0.90666667, 0.88      , 0.93333333, 0.94666667,\n",
       "        0.90666667, 0.96      , 0.94666667, 0.90666667, 0.93333333,\n",
       "        0.94666667, 0.92      , 0.93333333, 0.93333333, 0.92      ,\n",
       "        0.93333333, 0.93333333, 0.92      , 0.93333333, 0.93333333,\n",
       "        0.90666667, 0.93333333, 0.93333333, 0.89333333]),\n",
       " 'split4_test_score': array([0.92      , 0.92      , 0.89333333, 0.93333333, 0.92      ,\n",
       "        0.85333333, 0.90666667, 0.90666667, 0.88      , 0.92      ,\n",
       "        0.92      , 0.88      , 0.90666667, 0.90666667, 0.86666667,\n",
       "        0.90666667, 0.90666667, 0.86666667, 0.90666667, 0.89333333,\n",
       "        0.89333333, 0.92      , 0.90666667, 0.89333333]),\n",
       " 'split5_test_score': array([0.96      , 0.97333333, 0.92      , 0.97333333, 0.96      ,\n",
       "        0.93333333, 0.94666667, 0.94666667, 0.94666667, 0.96      ,\n",
       "        0.94666667, 0.93333333, 0.96      , 0.94666667, 0.92      ,\n",
       "        0.94666667, 0.96      , 0.93333333, 0.94666667, 0.94666667,\n",
       "        0.93333333, 0.93333333, 0.94666667, 0.93333333]),\n",
       " 'split6_test_score': array([0.86666667, 0.88      , 0.86666667, 0.90666667, 0.90666667,\n",
       "        0.88      , 0.90666667, 0.92      , 0.88      , 0.90666667,\n",
       "        0.90666667, 0.88      , 0.93333333, 0.93333333, 0.88      ,\n",
       "        0.90666667, 0.93333333, 0.85333333, 0.94666667, 0.93333333,\n",
       "        0.88      , 0.93333333, 0.92      , 0.86666667]),\n",
       " 'split7_test_score': array([0.90666667, 0.90666667, 0.89333333, 0.94666667, 0.89333333,\n",
       "        0.88      , 0.89333333, 0.92      , 0.89333333, 0.89333333,\n",
       "        0.89333333, 0.89333333, 0.89333333, 0.88      , 0.89333333,\n",
       "        0.88      , 0.89333333, 0.85333333, 0.88      , 0.89333333,\n",
       "        0.86666667, 0.89333333, 0.90666667, 0.86666667]),\n",
       " 'split8_test_score': array([0.90666667, 0.92      , 0.89333333, 0.97333333, 0.97333333,\n",
       "        0.94666667, 0.96      , 0.97333333, 0.96      , 0.94666667,\n",
       "        0.94666667, 0.96      , 0.93333333, 0.93333333, 0.96      ,\n",
       "        0.92      , 0.93333333, 0.97333333, 0.92      , 0.93333333,\n",
       "        0.94666667, 0.90666667, 0.90666667, 0.94666667]),\n",
       " 'split9_test_score': array([0.90666667, 0.92      , 0.94666667, 0.94666667, 0.94666667,\n",
       "        0.93333333, 0.97333333, 0.97333333, 0.96      , 1.        ,\n",
       "        0.97333333, 0.94666667, 0.98666667, 0.97333333, 0.97333333,\n",
       "        1.        , 0.98666667, 0.93333333, 1.        , 1.        ,\n",
       "        0.93333333, 1.        , 0.98666667, 0.92      ]),\n",
       " 'mean_test_score': array([0.916     , 0.92266667, 0.896     , 0.94533333, 0.93866667,\n",
       "        0.908     , 0.94133333, 0.94666667, 0.91866667, 0.94266667,\n",
       "        0.94      , 0.91866667, 0.94133333, 0.93733333, 0.90933333,\n",
       "        0.93733333, 0.94266667, 0.904     , 0.94133333, 0.94133333,\n",
       "        0.908     , 0.936     , 0.93466667, 0.89866667]),\n",
       " 'std_test_score': array([0.03268707, 0.03143954, 0.0244404 , 0.0256125 , 0.02933333,\n",
       "        0.02887521, 0.02809508, 0.02309401, 0.02887521, 0.03268707,\n",
       "        0.02476557, 0.0256125 , 0.02933333, 0.02735771, 0.03309246,\n",
       "        0.03479464, 0.02862788, 0.03761796, 0.03330666, 0.03109841,\n",
       "        0.0256125 , 0.03143954, 0.02629744, 0.02872088]),\n",
       " 'rank_test_score': array([18, 15, 24,  2, 10, 20,  5,  1, 16,  3,  9, 16,  5, 11, 19, 11,  3,\n",
       "        22,  5,  5, 20, 13, 14, 23], dtype=int32)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpTq3NBs_SK5"
   },
   "source": [
    "We can get even more advanced with our `GridSearchCV` options, because it can search over grids, and also over lists of grids (a list of dictionaries). This is useful when different pre-processing steps or models have different hyper-parameters. \n",
    "\n",
    "For example, say we wanted to tune whether the `MinMaxScaler` should scale between 0 and 1 or between -1 and 1, while also considering the case of using `StandardScaler`. We can't just add `feature_range` to the `param_grid` dictionary because `StandardScaler` doesn't have a `feature_range` parameter. \n",
    "\n",
    "Instead we can create a list of two grids: one grid that always uses `MinMaxScaler` and one that always uses `StandardScaler`. This is a bit of a contrived example, but once we know more models and transformers there will be plenty of cases where this comes in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Xa_MsRmq_Rue"
   },
   "outputs": [],
   "source": [
    "param_grid = [ # list of two dicts\n",
    "    # first dict always uses MinMaxScaler\n",
    "    {'scaler': [MinMaxScaler()],\n",
    "     # two options for feature_range:\n",
    "     'feature_range': [(0, 1), (-1, 1)], \n",
    "     'knn__n_neighbors': [1, 3, 5, 7, 9, 11]},\n",
    "    # second dict always uses StandardScaler\n",
    "    # there are no scaling options that we're tuning\n",
    "    {'scaler': [StandardScaler()], \n",
    "     'knn__n_neighbors': [1, 3, 5, 7, 9, 11]}   \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q271UKy5AIp9"
   },
   "source": [
    "Note, the values for scaler always need to be a list, even if it's a list with a single element. So we can't specify `'scaler': MinMaxScaler()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMgo3uKhAYaI"
   },
   "source": [
    "### Accessing attributes in grid-search pipeline\n",
    "\n",
    "We may want to access information about the model. \n",
    "\n",
    "For example, we can access the model fitted on the whole training+validation data using the `best_estimator_` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hciF6WBVArpP",
    "outputId": "51916870-5d84-4e6b-c1b8-752e9298778b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;knn&#x27;, KNeighborsClassifier())]),\n",
       "             param_grid={&#x27;knn__n_neighbors&#x27;: [1, 3, 5, 7, 9, 11, 13, 15],\n",
       "                         &#x27;scaler&#x27;: [MinMaxScaler(), StandardScaler(),\n",
       "                                    &#x27;passthrough&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;knn&#x27;, KNeighborsClassifier())]),\n",
       "             param_grid={&#x27;knn__n_neighbors&#x27;: [1, 3, 5, 7, 9, 11, 13, 15],\n",
       "                         &#x27;scaler&#x27;: [MinMaxScaler(), StandardScaler(),\n",
       "                                    &#x27;passthrough&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;knn&#x27;, KNeighborsClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('knn', KNeighborsClassifier())]),\n",
       "             param_grid={'knn__n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15],\n",
       "                         'scaler': [MinMaxScaler(), StandardScaler(),\n",
       "                                    'passthrough']})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KqGh4SdYAxP6",
    "outputId": "9a6b44ce-3e8f-4919-ca5f-b771ad9e821a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;knn&#x27;, KNeighborsClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;knn&#x27;, KNeighborsClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('knn', KNeighborsClassifier())])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ItiD_rAA2Xi"
   },
   "source": [
    "You can see that best estimator is a pipeline itself.  We can also access an individual step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CO_I1iMXA1I0",
    "outputId": "2c9ceb9f-0516-4790-d47b-30775840c294"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_['scaler']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjM-swmPBKRB"
   },
   "source": [
    "This is a scaler that was fit on the whole training+validation dataset. \n",
    "\n",
    "We can also access parameters that scaler uses, for example the min values used in the `MinMaxScaler` or the mean values used in the `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEuufGvUBSPd",
    "outputId": "53db4cd1-a39e-447c-88b1-04636118a5a5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.40896306,  0.36480057,  1.29951722, -0.0558038 ,  0.41304036,\n",
       "        0.58007339, -0.46107616,  0.50317085, -0.11268126, -0.09956297,\n",
       "        0.87089602, -0.71248985,  0.96428355, -1.07964908,  0.04909933,\n",
       "        1.03999126, -1.50948237, -0.05674912,  0.61672511,  0.49217227])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_['scaler'].mean_"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "p6.soln.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-un5550] *",
   "language": "python",
   "name": "conda-env-.conda-un5550-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
